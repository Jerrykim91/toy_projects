{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c844fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "fname = '1909.12744.pdf'\n",
    "fp = open(fname, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "598258d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PDFParser(fp)\n",
    "password = ''\n",
    "document = PDFDocument(parser, password)\n",
    "retstr = StringIO()\n",
    "if not document.is_extractable:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "rsrcmgr = PDFResourceManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33d2727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_type = ['LTTextBox', 'LTFigure', 'LTImage', 'LTCurve', 'LTRect']\n",
    "color = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (160, 32, 240)]\n",
    "\n",
    "draw_color = dict(zip(layout_type, color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "970cf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obj(lt_objs):\n",
    "\n",
    "    boxs = {x: [] for x in layout_type}\n",
    "    # loop over the object list\n",
    "    for obj in lt_objs:\n",
    "\n",
    "        if isinstance(obj, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            boxs['LTTextBox'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTFigure):\n",
    "#             boxs['LTFigure'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTImage):\n",
    "#             boxs['LTImage'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTCurve):\n",
    "#             boxs['LTCurve'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTRect):\n",
    "#             boxs['LTRect'].append(obj.bbox)\n",
    "        else:\n",
    "            pass\n",
    "            #raise\n",
    "    return boxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d75b442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_info_extract(objcet, parms):\n",
    "    if len(parms) ==2:\n",
    "        x,y = parms\n",
    "        Lic = list()\n",
    "        height = layout.bbox[3]\n",
    "        for num,obj in enumerate(objcet):\n",
    "            x_0 = obj.bbox[0] \n",
    "            if x_0 > x-10 and y-10 > x_0 : \n",
    "                x_0,y_0,x_1,y_1= obj.bbox\n",
    "                box = (x_0, height-y_1, x_1, height-y_0)\n",
    "                box_integer = tuple([round(jj) for jj in obj.bbox])\n",
    "                txt = obj.get_text().replace('\\n', '')\n",
    "                x1,y1,x2,y2 = box_integer\n",
    "                Lic.append((x1,y1,x2,y2,txt))\n",
    "        return Lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "869b2b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fname = '1909.12744.pdf'\n",
    "fp = open(fname, 'rb')\n",
    "parser = PDFParser(fp)\n",
    "password = ''\n",
    "document = PDFDocument(parser, password)\n",
    "retstr = StringIO()\n",
    "\n",
    "if not document.is_extractable:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "rsrcmgr = PDFResourceManager()\n",
    "\n",
    "layout_type = ['LTTextBox', 'LTFigure', 'LTImage', 'LTCurve', 'LTRect']\n",
    "color = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (160, 32, 240)]\n",
    "draw_color = dict(zip(layout_type, color))\n",
    "\n",
    "\n",
    "def parse_obj(lt_objs):\n",
    "    \n",
    "    boxs = {x: [] for x in layout_type}\n",
    "    for obj in lt_objs:\n",
    "\n",
    "        if isinstance(obj, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            boxs['LTTextBox'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTFigure):\n",
    "#             boxs['LTFigure'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTImage):\n",
    "#             boxs['LTImage'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTCurve):\n",
    "#             boxs['LTCurve'].append(obj.bbox)\n",
    "#         elif isinstance(obj, pdfminer.layout.LTRect):\n",
    "#             boxs['LTRect'].append(obj.bbox)\n",
    "        else:\n",
    "            pass\n",
    "            #raise\n",
    "    return boxs\n",
    "\n",
    "\n",
    "image = convert_from_path(fname)\n",
    "\n",
    "# Set parameters for analysis.\n",
    "laparams = LAParams()\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "page_boxs = []\n",
    "for page in PDFPage.create_pages(document):\n",
    "    interpreter.process_page(page)\n",
    "    \n",
    "    # receive the LTPage object for the page.\n",
    "    layout = device.get_result()\n",
    "    boxs = parse_obj(layout._objs)\n",
    "    page_sized = tuple([round(i) for i in layout.bbox])\n",
    "    height = page_sized[-1]\n",
    "    page_boxs.append((page_sized, boxs))\n",
    "    page_info = text_info_extract(layout,(72,307))\n",
    "    \n",
    "    break\n",
    "\n",
    "# page_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24542238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the use of BERT for Neural Machine Translation\n",
      "St´ephane ClinchantNAVER LABS Europe, Francestephane.clinchant@naverlabs.com\n",
      "Kweon Woo JungNAVER Corp.,South Koreakweonwoo.jung@navercorp.com\n",
      "Abstract\n",
      "Exploiting large pretrained models for var-ious NMT tasks have gained a lot of vis-In this work we studyibility recently.how BERT pretrained models could be ex-ploited for supervised Neural Machine Trans-lation. We compare various ways to in-tegrate pretrained BERT model with NMTmodel and study the impact of the mono-lingual data used for BERT training on theﬁnal translation quality. We use WMT-14English-German, IWSLT15 English-Germanand IWSLT14 English-Russian datasets forthese experiments. In addition to standard tasktest set evaluation, we perform evaluation onout-of-domain test sets and noise injected testsets, in order to assess how BERT pretrainedrepresentations affect model robustness.\n",
      "1 Introduction\n",
      "Pretrained Language Models (LM) such as ELMOand BERT (Peters et al., 2018; Devlin et al.,2018) have turned out to signiﬁcantly improvethe quality of several Natural Language Process-ing (NLP) tasks by transferring the prior knowl-edge learned from data-rich monolingual corporato data-poor NLP tasks such as question answer-ing, bio-medical information extraction and stan-dard benchmarks (Wang et al., 2018; Lee et al.,2019). In addition, it was shown that these rep-resentations contain syntactic and semantic infor-mation in different layers of the network (Tenneyet al., 2019). Therefore, using such pretrainedLMs for Neural Machine Translation (NMT) isappealing, and has been recently tried by severalpeople (Lample and Conneau, 2019; Edunov et al.,2019; Song et al., 2019).\n",
      "Unfortunately, the results of the\n",
      "above-mentioned works are not directly comparable toeach other as they used different methods, datasetsFurthermore, pretrained LMs haveand tasks.\n"
     ]
    }
   ],
   "source": [
    "def issue_area_replace(page_info):\n",
    "    # 데이터 정렬 \n",
    "    sample = np.array(page_info).reshape(-1,5)\n",
    "    df = pd.DataFrame(sample,columns=['x1','y1','x2','y2','text'])\n",
    "    df = df.astype({'x1':'int','y1':'int','x2':'int','y2':'int','text':'str'})\n",
    "    df = df.sort_values(by=['y1','y2','x1'],ascending=(False,False,True))\n",
    "    \n",
    "    # text replace \n",
    "    stmp = 0\n",
    "    txt = list()\n",
    "    overlap_key = df['y1'].value_counts()[df['y1'].value_counts().values > 1].keys()\n",
    "\n",
    "    for idx, val in zip(df['text'].index, df['y1']):\n",
    "        if val in overlap_key:\n",
    "            if stmp != val:\n",
    "                stmp = val\n",
    "                txt_smb = df[df['y1'] ==val]['text'].tolist()\n",
    "                join_txt = \" \".join(txt_smb)\n",
    "                txt.append(join_txt)\n",
    "            else:pass\n",
    "        else:\n",
    "            txt.append(df['text'][idx])\n",
    "    \n",
    "    return txt\n",
    "\n",
    "text_list = issue_area_replace(page_info)\n",
    "replace_text = \"\\n\".join(text_list)\n",
    "print(replace_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e172e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>757</td>\n",
       "      <td>460</td>\n",
       "      <td>772</td>\n",
       "      <td>On the use of BERT for Neural Machine Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>688</td>\n",
       "      <td>237</td>\n",
       "      <td>727</td>\n",
       "      <td>St´ephane ClinchantNAVER LABS Europe, Francest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>674</td>\n",
       "      <td>371</td>\n",
       "      <td>727</td>\n",
       "      <td>Kweon Woo JungNAVER Corp.,South Koreakweonwoo....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>606</td>\n",
       "      <td>203</td>\n",
       "      <td>618</td>\n",
       "      <td>Abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89</td>\n",
       "      <td>393</td>\n",
       "      <td>273</td>\n",
       "      <td>594</td>\n",
       "      <td>Exploiting large pretrained models for var-iou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72</td>\n",
       "      <td>368</td>\n",
       "      <td>78</td>\n",
       "      <td>380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90</td>\n",
       "      <td>368</td>\n",
       "      <td>155</td>\n",
       "      <td>380</td>\n",
       "      <td>Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>72</td>\n",
       "      <td>131</td>\n",
       "      <td>290</td>\n",
       "      <td>358</td>\n",
       "      <td>Pretrained Language Models (LM) such as ELMOan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>158</td>\n",
       "      <td>117</td>\n",
       "      <td>171</td>\n",
       "      <td>128</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>181</td>\n",
       "      <td>117</td>\n",
       "      <td>228</td>\n",
       "      <td>128</td>\n",
       "      <td>results of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>83</td>\n",
       "      <td>117</td>\n",
       "      <td>146</td>\n",
       "      <td>128</td>\n",
       "      <td>Unfortunately,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>238</td>\n",
       "      <td>117</td>\n",
       "      <td>251</td>\n",
       "      <td>128</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72</td>\n",
       "      <td>77</td>\n",
       "      <td>290</td>\n",
       "      <td>128</td>\n",
       "      <td>above-mentioned works are not directly compara...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1   y1   x2   y2                                               text\n",
       "0   138  757  460  772  On the use of BERT for Neural Machine Translation\n",
       "1    65  688  237  727  St´ephane ClinchantNAVER LABS Europe, Francest...\n",
       "2   226  674  371  727  Kweon Woo JungNAVER Corp.,South Koreakweonwoo....\n",
       "3   159  606  203  618                                           Abstract\n",
       "4    89  393  273  594  Exploiting large pretrained models for var-iou...\n",
       "5    72  368   78  380                                                  1\n",
       "6    90  368  155  380                                       Introduction\n",
       "7    72  131  290  358  Pretrained Language Models (LM) such as ELMOan...\n",
       "8   158  117  171  128                                                the\n",
       "9   181  117  228  128                                         results of\n",
       "10   83  117  146  128                                     Unfortunately,\n",
       "12  238  117  251  128                                                the\n",
       "11   72   77  290  128  above-mentioned works are not directly compara..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.array(page_info).reshape(-1,5)\n",
    "df = pd.DataFrame(sample,columns=['x1','y1','x2','y2','text'])\n",
    "df = df.astype({'x1':'int','y1':'int','x2':'int','y2':'int','text':'str'})\n",
    "df = df.sort_values(by=['y2','y1'],ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d2f8a",
   "metadata": {},
   "source": [
    "# 텍스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc09dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the use of BERT for Neural Machine Translation\n",
      "\n",
      "St´ephane Clinchant\n",
      "NAVER LABS Europe, France\n",
      "stephane.clinchant@naverlabs.com\n",
      "\n",
      "Kweon Woo Jung\n",
      "NAVER Corp.,\n",
      "South Korea\n",
      "kweonwoo.jung@navercorp.com\n",
      "\n",
      "Vassilina Nikoulina\n",
      "NAVER LABS Europe, France\n",
      "vassilina.nikoulina@naverlabs.com\n",
      "\n",
      "9\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      "p\n",
      "e\n",
      "S\n",
      "7\n",
      "2\n",
      "\n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "1\n",
      "v\n",
      "4\n",
      "4\n",
      "7\n",
      "2\n",
      "1\n",
      ".\n",
      "9\n",
      "0\n",
      "9\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Abstract\n",
      "\n",
      "Exploiting large pretrained models for var-\n",
      "ious NMT tasks have gained a lot of vis-\n",
      "In this work we study\n",
      "ibility recently.\n",
      "how BERT pretrained models could be ex-\n",
      "ploited for supervised Neural Machine Trans-\n",
      "lation. We compare various ways to in-\n",
      "tegrate pretrained BERT model with NMT\n",
      "model and study the impact of the mono-\n",
      "lingual data used for BERT training on the\n",
      "ﬁnal translation quality. We use WMT-14\n",
      "English-German, IWSLT15 English-German\n",
      "and IWSLT14 English-Russian datasets for\n",
      "these experiments. In addition to standard task\n",
      "test set evaluation, we perform evaluation on\n",
      "out-of-domain test sets and noise injected test\n",
      "sets, in order to assess how BERT pretrained\n",
      "representations affect model robustness.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Pretrained Language Models (LM) such as ELMO\n",
      "and BERT (Peters et al., 2018; Devlin et al.,\n",
      "2018) have turned out to signiﬁcantly improve\n",
      "the quality of several Natural Language Process-\n",
      "ing (NLP) tasks by transferring the prior knowl-\n",
      "edge learned from data-rich monolingual corpora\n",
      "to data-poor NLP tasks such as question answer-\n",
      "ing, bio-medical information extraction and stan-\n",
      "dard benchmarks (Wang et al., 2018; Lee et al.,\n",
      "2019). In addition, it was shown that these rep-\n",
      "resentations contain syntactic and semantic infor-\n",
      "mation in different layers of the network (Tenney\n",
      "et al., 2019). Therefore, using such pretrained\n",
      "LMs for Neural Machine Translation (NMT) is\n",
      "appealing, and has been recently tried by several\n",
      "people (Lample and Conneau, 2019; Edunov et al.,\n",
      "2019; Song et al., 2019).\n",
      "\n",
      "the\n",
      "\n",
      "results of\n",
      "\n",
      "Unfortunately,\n",
      "\n",
      "above-\n",
      "mentioned works are not directly comparable to\n",
      "each other as they used different methods, datasets\n",
      "Furthermore, pretrained LMs have\n",
      "and tasks.\n",
      "\n",
      "the\n",
      "\n",
      "mostly shown improvements in low-resource or\n",
      "unsupervised NMT settings, and has been little\n",
      "studied in standard supervised scenario with rea-\n",
      "sonable amount of data available.\n",
      "\n",
      "Current state of the art NMT models rely on\n",
      "the Transformer model (Vaswani et al., 2017), a\n",
      "feed-forward network relying on attention mech-\n",
      "anism, which has surpassed prior state of the art\n",
      "architecture based on recurrent neural nets (Bah-\n",
      "danau et al., 2014; Sutskever et al., 2014). Beyond\n",
      "machine translation, the transformer models have\n",
      "been reused to learn bi-directional language mod-\n",
      "els on large text corpora. The BERT model (De-\n",
      "vlin et al., 2018) consists in a transformer model\n",
      "aiming at solving a masked language modelling\n",
      "task, namely correctly predicting a masked word\n",
      "from its context, and a next sentence prediction\n",
      "task to decide whether two sentences are consecu-\n",
      "tive or not. In this work, we study how pretrained\n",
      "BERT models can be exploited for transformer-\n",
      "based NMT, thus exploiting the fact that they rely\n",
      "on the same architecture.\n",
      "\n",
      "The objective of this work is twofold. On one\n",
      "hand, we wish to perform systematic comparisons\n",
      "of different BERT+NMT architectures for stan-\n",
      "dard supervised NMT. In addition, we argue that\n",
      "the beneﬁts of using pretrained representations has\n",
      "been overlooked in previous studies and should\n",
      "be assessed beyond BLEU scores on in-domain\n",
      "In fact, LMs trained on huge datasets\n",
      "datasets.\n",
      "have the potentials of being more robust in general\n",
      "and improve the performance for domain adapta-\n",
      "tion in MT.\n",
      "\n",
      "In this study, we compare different ways to train\n",
      "and reuse BERT for NMT. For instance, we show\n",
      "that BERT can be trained only with a masked LM\n",
      "task on the NMT source corpora and yield signif-\n",
      "icant improvement over the baseline. In addition,\n",
      "the models robustness is analyzed thanks to syn-\n",
      "thetic noise.\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fThe paper is organized as follows.\n",
      "\n",
      "In section\n",
      "2, we review relevant state of the art. Section 3\n",
      "enumerates different models we experiment with.\n",
      "Finally section 4 and 5 present our results before\n",
      "discussing the main contributions of this work in\n",
      "section 6.\n",
      "\n",
      "2 Related Works\n",
      "\n",
      "The seminal work of (Bengio et al., 2003; Col-\n",
      "lobert and Weston, 2008) were one of the ﬁrst to\n",
      "show that neural nets could learn word represen-\n",
      "tations useful in a variety of NLP tasks, paving\n",
      "the way for the word embedding era thanks to\n",
      "word2vec (Mikolov et al., 2013) and its vari-\n",
      "ants (Pennington et al., 2014; Levy and Goldberg,\n",
      "2014).\n",
      "\n",
      "With the recent advances and boost in perfor-\n",
      "mance of neural nets, ELMO (Peters et al., 2018)\n",
      "employed a Bi-LSTM network for language mod-\n",
      "elling and proposed to combine the different net-\n",
      "work layers to obtain effective word representa-\n",
      "tions. Shortly after the publication of ELMO,\n",
      "the BERT model (Devlin et al., 2018) was shown\n",
      "to have outstanding performance in various NLP\n",
      "tasks. Furthermore, the BERT model was reﬁned\n",
      "in (Baevski et al., 2019) where the transformer\n",
      "self-attention mechanism is replaced by two di-\n",
      "rectional self-attention blocks: a left-to-right and\n",
      "right-to-left blocks are combined to predict the\n",
      "masked tokens.\n",
      "\n",
      "With respect to NMT, backtranslation (Sennrich\n",
      "et al., 2016a) is up to now one of the most effective\n",
      "ways to exploit large monolingual data. However,\n",
      "backtranslation has the drawback of being only\n",
      "applicable for target language data augmentation,\n",
      "while pretrained LMs can be used both for source\n",
      "and target language (independently (Edunov et al.,\n",
      "2019) or jointly (Lample and Conneau, 2019;\n",
      "Song et al., 2019)).\n",
      "\n",
      "Lample and Conneau (2019) initializes the\n",
      "entire encoder and decoder with a pretrained\n",
      "MaskLM or Crosslingual MaskLM language mod-\n",
      "els trained on multilingual corpora. Such ini-\n",
      "tialization proved to be beneﬁcial for unsuper-\n",
      "vised machine translation, but also for English-\n",
      "Romanian supervised MT, bringing additional im-\n",
      "provements over standard backtranslation with\n",
      "MLM initialization.\n",
      "\n",
      "Edunov et al. (2019) uses ELMO (Peters et al.,\n",
      "2018) language model to set the word embed-\n",
      "the\n",
      "dings layer in NMT model.\n",
      "\n",
      "In addition,\n",
      "\n",
      "ELMO embedding are compared with the cloze-\n",
      "style BERT (Baevski et al., 2019) ones. The em-\n",
      "bedding network parameters are then either ﬁxed,\n",
      "or ﬁne-tuned. This work shows improvements on\n",
      "English-German and English-Turkish translation\n",
      "tasks when using pretrained language model for\n",
      "source word embedding initialization. However,\n",
      "the results are less clear when reusing embedding\n",
      "on the target language side.\n",
      "\n",
      "Futhermore, Song et al.\n",
      "\n",
      "(2019) goes one\n",
      "step further and proposes Masked Sequence-to-\n",
      "Sequence pretraining method. Rather than mask-\n",
      "ing a single token, it masks a sequence of token\n",
      "in the encoder and recovers them in the decoder.\n",
      "This model has shown new state of the art for un-\n",
      "supervised machine translation.\n",
      "\n",
      "Our work is an attempt to perform systematic\n",
      "comparison on some of the aforementioned archi-\n",
      "tectures that incorporate pretrained LM in NMT\n",
      "model, concentrating on BERT pretrained LM rep-\n",
      "resentations applied on supervised machine trans-\n",
      "lation. However, we restrict ourselves to encoder\n",
      "part only, and leave the decoder initialization for\n",
      "future work.\n",
      "\n",
      "Regarding robustness, several recent studies\n",
      "(Karpukhin et al., 2019; Vaibhav et al., 2019) have\n",
      "tackled robustness issues with data augmentation.\n",
      "In this work, we study whether the robustness\n",
      "problem can be addressed at the model level rather\n",
      "than at data level. Michel et al. (2019) address ro-\n",
      "bustness problem with generative adversarial net-\n",
      "works. This method, as well as data augmentation\n",
      "methods are complementary to our work and we\n",
      "believe that they address different issues of robust-\n",
      "ness.\n",
      "\n",
      "3 Methods\n",
      "\n",
      "Typical NMT model adopts the encoder-decoder\n",
      "architecture where the encoder forms contextual-\n",
      "ized word embedding from a source sentence and\n",
      "the decoder generates a target translation from left\n",
      "to right.\n",
      "\n",
      "Pretrained LM, namely BERT, can inject prior\n",
      "knowledge on the encoder part of NMT, providing\n",
      "rich contextualized word embedding learned from\n",
      "large monolingual corpus. Moreover, pretrained\n",
      "LMs can be trained once, and reused for different\n",
      "language pairs1.\n",
      "\n",
      "1As opposed to backtranslation techniques which requires\n",
      "\n",
      "full NMT model retraining\n",
      "\n",
      "\fIn this study, we focus on reusing BERT mod-\n",
      "els for the NMT encoder2. We will compare the\n",
      "following models:\n",
      "\n",
      "• Baseline:. A transformer-big model with\n",
      "shared decoder input-output embedding pa-\n",
      "rameters.\n",
      "\n",
      "• Embedding (Emb):\n",
      "\n",
      "The baseline model\n",
      "where the embedding layer is replaced by the\n",
      "BERT parameters (thus having 6 + 6 encoder\n",
      "layers). The model is then ﬁne tuned simi-\n",
      "lar to the ELMO setting from (Edunov et al.,\n",
      "2019)\n",
      "\n",
      "• Fine-Tuning (FT): The baseline model with\n",
      "the encoder initialized by the BERT parame-\n",
      "ters as in Lample and Conneau (2019)\n",
      "\n",
      "• Freeze: The baseline model with the en-\n",
      "coder initialized by the BERT parameters and\n",
      "frozen. This means that the whole encoder\n",
      "has been trained in purely monolingual set-\n",
      "tings, and only parameters responsible for the\n",
      "translation belong to the attention and de-\n",
      "coder models.\n",
      "\n",
      "We exploit the fact that BERT uses the same\n",
      "architecture as NMT encoder which allows us to\n",
      "initialize NMT encoder with BERT pretrained pa-\n",
      "rameters. BERT pretraining has two advantages\n",
      "over NMT training:\n",
      "\n",
      "• it solves a simpler (monolingual) task of\n",
      "‘source sentence encoding’, compared to\n",
      "NMT (bilingual task) which has to ‘encode\n",
      "source sentence information’, and ‘translate\n",
      "into a target language’.\n",
      "\n",
      "• it has a possibility to exploit much larger\n",
      "data, while NMT encoder is limited to source\n",
      "side of parallel corpus only.\n",
      "\n",
      "Even though the role of NMT encoder may go\n",
      "beyond source sentence encoding (nothing pre-\n",
      "vents the model from encoding ‘translation re-\n",
      "lated’ information at the encoder level), better ini-\n",
      "tialization of encoder with BERT pretrained LM\n",
      "allows for faster NMT learning. Comparing set-\n",
      "tings where we freeze BERT parameters against\n",
      "ﬁne-tuning BERT allows to shed some light on\n",
      "the capacity of the encoder/decoder model to learn\n",
      "‘translation-related’ information.\n",
      "\n",
      "2Similar approach can be applied on the target language\n",
      "\n",
      "but we leave it for future work.\n",
      "\n",
      "Moreover, since the BERT models are trained\n",
      "to predict missing tokens from their context, their\n",
      "representations may also be more robust to miss-\n",
      "ing tokens or noisy inputs. We perform extensive\n",
      "robustness study at section 4 verifying this hypoth-\n",
      "esis.\n",
      "\n",
      "Finally,\n",
      "\n",
      "language models trained on huge\n",
      "datasets have the potentials of being more robust\n",
      "in general and improve the performance for do-\n",
      "main adaptation in MT. We therefore compare\n",
      "BERT models trained on different datasets, and\n",
      "perform evaluation on related test sets in order to\n",
      "assess the capacity of pretrained LMs on domain\n",
      "adaptation.\n",
      "\n",
      "4 WMT experiments\n",
      "\n",
      "4.1 Preprocessing\n",
      "\n",
      "We learn BPE (Sennrich et al., 2016b) model with\n",
      "32K split operations on the concatenation of Wiki\n",
      "and News corpus. This model is used both for Pre-\n",
      "trained LM subwords splitting and NMT source\n",
      "(English) side subwords splitting. German side of\n",
      "NMT has been processed with 32K BPE model\n",
      "learnt on target part of parallel corpus only. Please\n",
      "note, that this is different from standard settings\n",
      "for WMT En-De experiments, which usually uses\n",
      "joint BPE learning and shared source-target em-\n",
      "beddings. We do not adopt standard settings since\n",
      "it contradicts our original motivation for using pre-\n",
      "trained LM: English LM is learnt once and reused\n",
      "for different language pairs.\n",
      "\n",
      "4.2 Training\n",
      "\n",
      "BERT For pretraining BERT models, we use\n",
      "three different monolingual corpora of different\n",
      "sizes and different domains. Table 1 summarizes\n",
      "the statistics of these three monolingual corpora.\n",
      "\n",
      "• NMT-src: source part of our parallel corpus\n",
      "\n",
      "that is used for NMT model training.\n",
      "\n",
      "• Wiki: English wikipedia dump\n",
      "\n",
      "• News:\n",
      "\n",
      "concatenation of 70M samples\n",
      "from ”News Discussion”, ”News Crawl”\n",
      "and ”Common Crawl” English monolingual\n",
      "datasets distributed by WMT-2019 shared\n",
      "task3. This resulted in total 210M samples.\n",
      "\n",
      "The motivation of using NMT-src is to test\n",
      "whether the resulting NMT model is more robust\n",
      "\n",
      "3http://www.statmt.org/wmt19/translation-task.html\n",
      "\n",
      "\fNMT-src\n",
      "Wiki\n",
      "News\n",
      "\n",
      "Lines Tokens\n",
      "4.5M 104M\n",
      "72M 2086M\n",
      "210M 3657M\n",
      "\n",
      "Table 1: Monolingual (English) training data\n",
      "\n",
      "after having being trained on the source corpora.\n",
      "The Wiki corpora is bigger than the NMT-src but\n",
      "could be classiﬁed as out-of-domain compared to\n",
      "news dataset. Finally, the news dataset is the\n",
      "biggest one and consists mostly of in-domain data.\n",
      "In all of our experiments, we only consider us-\n",
      "ing the masked LM task for BERT as the next sen-\n",
      "tence prediction tasks put restrictions on possible\n",
      "data to use. We closely follow the masked LM\n",
      "task described in (Devlin et al., 2018) with few ad-\n",
      "justments optimized for downstream NMT train-\n",
      "ing. We use frequency based sampling (Lample\n",
      "and Conneau, 2019) in choosing 15% of tokens to\n",
      "mask, instead of uniformly sampling. Instead of\n",
      "MASK token we used UNK token hoping that thus\n",
      "trained model will learn certain representation for\n",
      "unknowns that could be exploited by NMT model.\n",
      "Warm-up learning scheme described in (Vaswani\n",
      "et al., 2017) results in faster convergence than\n",
      "linear decaying learning rate. The batch size of\n",
      "64000 tokens per batch is used, with maximum to-\n",
      "ken length of 250, half the original value, as we in-\n",
      "put single sentence only. We do not use [CLS] to-\n",
      "ken in the encoder side, as attention mechanism in\n",
      "NMT task can extract necessary information from\n",
      "token-level representations. The BERT model is\n",
      "equivalent to the encoder side of Transformer Big\n",
      "model. We train BERT model up to 200k iterations\n",
      "until the accuracy for masked LM on development\n",
      "saturates.\n",
      "\n",
      "NMT For NMT system training, we use WMT-\n",
      "14 English-German dataset.\n",
      "\n",
      "We use Transformer-Big as our baseline model.\n",
      "We share input embedding and output embed-\n",
      "ding parameters just before softmax on the de-\n",
      "coder side. Warm up learning scheme is used\n",
      "with warm-up steps of 4000. We use batch size\n",
      "of 32000 tokens per batch. Dropout of 0.3 is ap-\n",
      "plied to residual connections, and no dropout is\n",
      "applied in attention layers. We decode with beam\n",
      "size 4 with length penalty described in Wu et al.\n",
      "(2016). We conduct model selection with perplex-\n",
      "ity on development set. We average 5 checkpoints\n",
      "around lowest perplexity.\n",
      "\n",
      "news14\n",
      "news18\n",
      "iwslt15\n",
      "OpenSub\n",
      "KDE\n",
      "wiki\n",
      "\n",
      "Lines Tok/line (en/de)\n",
      "3003\n",
      "2997\n",
      "2385\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "\n",
      "19.7/18.3\n",
      "19.5/18.3\n",
      "16.4/15.4\n",
      "6.3/5.5\n",
      "8/7.7\n",
      "17.7/15.5\n",
      "\n",
      "In/Out of Domain test sets.\n",
      "\n",
      "Table 2:\n",
      "news14 and\n",
      "news18 are test sets from WMT-14 and WMT-18 news\n",
      "translation shared task.\n",
      "test set from IWSLT-\n",
      "iwslt:\n",
      "15 MT Track4. Wiki is randomly 5K sampled from\n",
      "parallel Wikipedia distributed by OPUS5, OpenSub,\n",
      "KDE and Wiki are randomly 5K sampled from paral-\n",
      "lel Wikipedia, Open Subtitles and KDE corpora dis-\n",
      "tributed by OPUS6\n",
      "\n",
      "4.3 Evaluation\n",
      "\n",
      "We believe that the impact of pretrained LM in\n",
      "NMT model can not be measured by BLEU per-\n",
      "formance on in-domain test set only. Therefore\n",
      "we introduce additional evaluation that allows to\n",
      "measure the impact of LM pretraining on different\n",
      "out-of-domain tests. We also propose an evalua-\n",
      "tion procedure to evaluate the robustness to vari-\n",
      "ous types of noise for our models.\n",
      "\n",
      "Domain Besides standard WMT-14 news test\n",
      "set, models are evaluated on additional test sets\n",
      "given by Table 2. We include two in-domain\n",
      "(news) test sets, as well as additional out-of-\n",
      "domain test sets described in Table 2.\n",
      "\n",
      "Noise robustness. For robustness evaluation, we\n",
      "introduce different type of noise to the standard\n",
      "news14 test set:\n",
      "\n",
      "Typos: Similar to Karpukhin et al. (2019), we\n",
      "add synthetic noise to the test set by randomly (1)\n",
      "swapping characters (chswap), (2) randomly in-\n",
      "serting or deleting characters (chrand), (3) upper-\n",
      "casing words (up). These test sets translations are\n",
      "evaluated against the golden news14 reference.\n",
      "\n",
      "Unk: An unknown character is introduced at the\n",
      "beginning (noted UNK.S) or at the end of the sen-\n",
      "tence (noted UNK.E) before a punctuation symbol\n",
      "if any (this unknown character could be thought\n",
      "as as an unknown emoji, a character in different\n",
      "script, a rare unicode character). This token is in-\n",
      "troduced both for source and target sentence, and\n",
      "the evaluation is performed with the augmented-\n",
      "reference.\n",
      "\n",
      "Intuitively, we expect the model to simply copy\n",
      "UNK token and proceed to the remaining tokens.\n",
      "\n",
      "\fInterestingly, this simple test seems to produce\n",
      "poor translations, therefore puzzling the attention\n",
      "and decoding process a lot. Table 3 gives an ex-\n",
      "ample of such translations for baseline model7.\n",
      "\n",
      "Since the tasks are correlated, a better model\n",
      "might be better on noisy test sets as it behaves bet-\n",
      "ter in general. If we want to test that some models\n",
      "are indeed better, we need to disentangle this ef-\n",
      "fect and show that the gain in performance is not\n",
      "just a random effect. A proper way would be to\n",
      "compute the BLEU correlation between the orig-\n",
      "inal test set and the noisy versions but it would\n",
      "require a larger set of models for an accurate cor-\n",
      "relation estimation.\n",
      "\n",
      "∆(chrF) : We propose to look at the distribu-\n",
      "tion of the difference of sentence charf between\n",
      "the noisy test set and the original test set. Indeed,\n",
      "looking at BLEU delta may not provide enough in-\n",
      "formation since it is corpus-level metric. Ideally,\n",
      "we would like to measure a number of sentences\n",
      "or a margin for which we observe an ‘important\n",
      "decrease’ in translation quality. According to Ma\n",
      "et al. (2018); Bojar et al. (2017), sentence level\n",
      "chrF achieves good correlation with human judge-\n",
      "ments for En-De news translations.\n",
      "\n",
      "More formally, let s be a sentence from the stan-\n",
      "dard news14 test set, n a noise operation, m a\n",
      "translation model and r the reference sentence8:\n",
      "\n",
      "∆(chrF)(m, n, s) = chrF(m(n(s)), r) −\n",
      "\n",
      "chrF(m(s), r)\n",
      "\n",
      "(1)\n",
      "\n",
      "In the analysis, we will report the distribution of\n",
      "∆(chrF) and its mean value as a summary. If a\n",
      "model is good at dealing with noise, then the pro-\n",
      "duced sentence will be similar to the one produced\n",
      "by the noise-free input sentence. Therefore, the\n",
      "∆(chrF) will be closer to zero.\n",
      "\n",
      "4.4 Results\n",
      "\n",
      "Table 4 presents the results of our experiments.\n",
      "As expected, freezing the encoder with BERT pa-\n",
      "rameters lead to a signiﬁcant decrease in transla-\n",
      "tion quality. However, other BERT+NMT archi-\n",
      "tectures mostly improve over the baseline both on\n",
      "in-domain and out-of-domain test sets. We con-\n",
      "clude, that the information encoded by BERT is\n",
      "useful but not sufﬁcient to perform the translation\n",
      "\n",
      "7Output for (UNK.S+src) input is not an error, the model\n",
      "\n",
      "does produces an English sentence!\n",
      "\n",
      "8In the case of UNK transformation,\n",
      "\n",
      "the reference is\n",
      "\n",
      "changed but we omit that to simplify the notation.\n",
      "\n",
      "task. We believe, that the role of the NMT encoder\n",
      "is to encode both information speciﬁc to source\n",
      "sentence, but also information speciﬁc to the tar-\n",
      "get sentence (which is missing in BERT training).\n",
      "Next, we observe that even NMTSrc.FT (NMT\n",
      "encoder is initialized with BERT trained on source\n",
      "part of parallel corpus) improves over the baseline.\n",
      "Note that this model uses the same amount of data\n",
      "as the baseline. BERT task is simpler compared to\n",
      "the task of the NMT encoder, but it is still related,\n",
      "thus BERT pretraining allows for a better initial-\n",
      "ization point for NMT model.\n",
      "\n",
      "When using more data for BERT training\n",
      "(Wiki.FT and News.FT), we gain even more im-\n",
      "provements over the baseline.\n",
      "\n",
      "Finally, we observe comparable results for\n",
      "News.Emb and News.FT (the difference in BLEU\n",
      "doesn’t exceed 0.3 points, being higher\n",
      "for\n",
      "News.FT on in-domain tests, and News.Emb for\n",
      "out-of-domain tests). Although News.FT conﬁg-\n",
      "uration keeps the size of the model same as stan-\n",
      "dard NMT system, News.Emb adds BERT param-\n",
      "eters to NMT parameters which doubles the size\n",
      "of NMT encoder. Additional encoder layers intro-\n",
      "duced in News.Emb does not add signiﬁcant value.\n",
      "\n",
      "4.5 Robustness analysis\n",
      "\n",
      "Table 5 reports BLEU scores for the noisy test sets\n",
      "(described in section 4.3). As expected, we ob-\n",
      "serve an important drop in BLEU scores due to\n",
      "the introduced noise. We observe that most pre-\n",
      "trained BERT models have better BLEU scores\n",
      "compared to baseline for all type of noise (except\n",
      "NMTSrc.FT which suffers more from unknown\n",
      "token introduction in the end of the sentence com-\n",
      "pared to the Baseline). However, these results are\n",
      "not enough to conclude, whether higher BLEU\n",
      "scores of BERT-augmented models are due to bet-\n",
      "ter robustness, or simply because these models are\n",
      "slightly better than the baseline in general.\n",
      "\n",
      "This is why ﬁgure 1 reports the mean ∆(chrF)\n",
      "for several models. ∆(chrF) scores for UNK\n",
      "tests show that BERT models are not better than\n",
      "expected. However, for chswap, chrand, upper,\n",
      "the BERT models have a slightly lower ∆(chrF).\n",
      "Based on these results, we conclude that pretrain-\n",
      "ing the encoder with a masked LM task does not\n",
      "really bring improvement in terms of robustness\n",
      "to unknowns. It seems that BERT does yield im-\n",
      "provement for NMT as a better initialization for\n",
      "\n",
      "\fsource sentence\n",
      "\n",
      "translation(src)\n",
      "\n",
      "translation(UNK.S + src)\n",
      "\n",
      "”In home cooking, there is much to be discovered - with a few minor\n",
      "tweaks you can achieve good, if not sometimes better results,” said Proctor.\n",
      "”Beim Kochen zu Hause gibt es viel zu entdecken - mit ein paar kleinen\n",
      "nderungen kann man gute, wenn nicht sogar manchmal bessere\n",
      "Ergebnisse erzielen”, sagte Proktor.\n",
      "• ”In home cooking, there is much to be discovered - with a few minor\n",
      "tweaks you can achieve good, if not sometimes better results”, sagte Proktor.\n",
      "\n",
      "Table 3: Example of a poor translation when adding unknown token to source sentences (translation done with a\n",
      "baseline transformer model)\n",
      "\n",
      "Baseline\n",
      "NMTsrc.FT\n",
      "Wiki.FT\n",
      "News.FT\n",
      "News.Emb\n",
      "News.Freeze\n",
      "\n",
      "news14\n",
      "27.3\n",
      "27.7\n",
      "27.7\n",
      "27.9\n",
      "27.7\n",
      "23.6\n",
      "\n",
      "news18\n",
      "39.5\n",
      "40.1\n",
      "40.6\n",
      "40.2\n",
      "39.9\n",
      "35.5\n",
      "\n",
      "iwslt15 wiki\n",
      "17.6\n",
      "18.3\n",
      "18.4\n",
      "18.8\n",
      "18.9\n",
      "15.0\n",
      "\n",
      "28.9\n",
      "28.7\n",
      "28.7\n",
      "29.1\n",
      "29.3\n",
      "26.5\n",
      "\n",
      "kde OpenSub\n",
      "18.1\n",
      "18.4\n",
      "19.0\n",
      "17.9\n",
      "18.2\n",
      "15.1\n",
      "\n",
      "15.3\n",
      "15.3\n",
      "15.4\n",
      "15.7\n",
      "16.0\n",
      "13.8\n",
      "\n",
      "Table 4: FT: initialize NMT encoder with BERT and ﬁnetune; Freeze: ﬁx NMT encoder parameters to BERT\n",
      "parameters; Emb: ﬁx encoder embeddding layer with BERT contextual word embeddings.\n",
      "\n",
      "German IWSLT 20159\n",
      "and English-Russian\n",
      "IWSLT 201410 MT track datasets. These are\n",
      "pretty small datasets (compared to previous exper-\n",
      "iments) which contain around 200K parallel sen-\n",
      "tences each.\n",
      "\n",
      "5.1 Experimental settings\n",
      "\n",
      "In these experiments we (1) reuse pretrained\n",
      "BERT models from previous experiments or (2)\n",
      "train IWSLT BERT model. IWSLT BERT model\n",
      "is trained on the concatenation of all the data avail-\n",
      "able at IWSLT 2014-2018 campaigns. After ﬁlter-\n",
      "ing out all the duplicates it contains around 780K\n",
      "sentences and 13.8M tokens.\n",
      "\n",
      "We considered various settings for IWSLT base-\n",
      "line. First, for source side of the dataset, we\n",
      "took 10K BPE merge operations, where BPE\n",
      "model was trained (1) either on the source side of\n",
      "NMT data only, or (2) on all monolingual English\n",
      "IWSLT data. Target side BPE uses 10K merge\n",
      "operations trained on the target side of the NMT\n",
      "dataset in all the IWSLST experiments. In our ﬁrst\n",
      "set of experiments, BPE model learnt on source\n",
      "data only lead to similar translation performance\n",
      "as BPE model learnt on all IWSLT English data.\n",
      "Therefore, in what follows we report results only\n",
      "\n",
      "Figure 1: Mean ∆(chrF) for several noisy test set and\n",
      "models. For the UNK test, the BERT models are sim-\n",
      "ilar or worst than the basline. For the chrand, chswap,\n",
      "upper, the BERT models are slightly better.\n",
      "\n",
      "NMT encoders but the full potential of masked\n",
      "LM task is not fully exploited for NMT.\n",
      "\n",
      "5\n",
      "\n",
      "IWSLT experiments\n",
      "\n",
      "to explore the potential of masked\n",
      "In order\n",
      "LM encoder pretraining for NMT in lower re-\n",
      "source settings, we train NMT models on English-\n",
      "\n",
      "9https://sites.google.com/site/iwsltevaluation2015/mt-\n",
      "\n",
      "track\n",
      "\n",
      "10https://sites.google.com/site/iwsltevaluation2014/mt-\n",
      "\n",
      "track\n",
      "\n",
      "\fModels\n",
      "Baseline\n",
      "NMTsrc.FT\n",
      "Wiki.FT\n",
      "News.FT\n",
      "News.Emb\n",
      "\n",
      "news14 +UNK.S +UNK.E +chswap +chrand\n",
      "24.4\n",
      "22.9\n",
      "24.9\n",
      "24.9\n",
      "24.8\n",
      "\n",
      "24.2\n",
      "24.4\n",
      "24.4\n",
      "24.5\n",
      "24.6\n",
      "\n",
      "27.3\n",
      "27.7\n",
      "27.7\n",
      "27.9\n",
      "27.7\n",
      "\n",
      "24.8\n",
      "24.9\n",
      "25.8\n",
      "24.9\n",
      "24.7\n",
      "\n",
      "24.7\n",
      "25.2\n",
      "24.9\n",
      "25.3\n",
      "25.3\n",
      "\n",
      "+up\n",
      "23.5\n",
      "24.5\n",
      "24.4\n",
      "24.5\n",
      "24.2\n",
      "\n",
      "Table 5: Robustness tests. BLEU scores for clean and ’noisiﬁed’ (with different noise type) news14 testset.\n",
      "\n",
      "for the latter (referred as bpe10k).\n",
      "\n",
      "NMT model training on IWSLT datasets with\n",
      "Transformer Big architecture on IWSLT data has\n",
      "diverged both for en-de and en-ru dataset. There-\n",
      "fore we use Transformer Base (tbase) architecture\n",
      "as a baseline model for these experiments. IWSLT\n",
      "BERT model is also based on tbase architecture\n",
      "described in Vaswani et al. (2017) and for the rest\n",
      "follows same training procedure as described in\n",
      "the section 4.\n",
      "\n",
      "In order to explore the potential of single pre-\n",
      "trained model for all language pairs/datasets we\n",
      "try to reuse Wiki and News pretrained BERT mod-\n",
      "els from previous experiments for encoder initial-\n",
      "ization of NMT model. However, in the previous\n",
      "experiments, our pretrained BERT models used\n",
      "32K BPE vocabulary and Transformer Big (tbig)\n",
      "architecture which means that we have to reuse the\n",
      "same settings for the encoder trained on IWSLT\n",
      "dataset. It has been shown by Ding et al. (2019),\n",
      "these are not optimal settings for IWSLT train-\n",
      "ing because it leads to too many parameters for\n",
      "the amount of data available. Therefore, in or-\n",
      "der to reduce the amount of the parameters of the\n",
      "model, we also consider the case where we re-\n",
      "duce the amount of the decoder layers from 6 to\n",
      "3 (tbig.dec3).\n",
      "\n",
      "tbase.bpe10k\n",
      "tbase.dec3.bpe10k\n",
      "\n",
      "IWSLT.FT.tbase.bpe10k\n",
      "IWSLT.FT.tbase.dec3.bpe10k\n",
      "Wiki.FT.tbig.bpe32k\n",
      "Wiki.FT.tbig.dec3.bpe32k\n",
      "News.FT.tbig.bpe32k\n",
      "News.FT.tbig.dec3.bpe32k\n",
      "\n",
      "en-de\n",
      "\n",
      "en-ru\n",
      "\n",
      "Baseline\n",
      "\n",
      "25.9\n",
      "9.6\n",
      "16.3\n",
      "26.4\n",
      "BERT+NMT\n",
      "17.6\n",
      "27.4\n",
      "18.1\n",
      "27.2\n",
      "17.6\n",
      "26.9\n",
      "27.7\n",
      "17.8\n",
      "17.9\n",
      "27.1\n",
      "17.9\n",
      "27.6\n",
      "\n",
      "Table 6: IWSLT dataset results. IWSLT.FT: encoder\n",
      "is initialised with BERT model trained on IWSLT\n",
      "data; tbase/tbig: transformer base/big architecture for\n",
      "NMT model; dec3: decoder layers reduced for 6 to\n",
      "3; bpe10k/bpe32k : amount of BPE merge operations\n",
      "used for source language, learnt on the same dataset as\n",
      "BERT model (IWSLT or Wiki+News).\n",
      "\n",
      "We do not observe big difference between\n",
      "IWSLT pretrained model and News/Wiki pre-\n",
      "trained model. We therefore may assume that\n",
      "News/Wiki BERT model can be considered as\n",
      "”general” English pretrained encoder, and be used\n",
      "as a good starting point in any new model trans-\n",
      "lating from English (no matter target language or\n",
      "domain).\n",
      "\n",
      "5.2 Results\n",
      "\n",
      "6 Discussion\n",
      "\n",
      "Table 6 reports the results of different sets of the\n",
      "experiments on IWSLT data. First, we observe\n",
      "that BERT pretrained model improves over the\n",
      "baseline, in any settings (BPE vocabulary, model\n",
      "architecture, dataset used for pretraining).\n",
      "In\n",
      "particular, it is interesting to mention that with-\n",
      "out pretraining, both tbig.bpe32k and tbig.bpe10k\n",
      "models diverge when trained on IWSLT. How-\n",
      "ever, BERT pretraining gives a better initializa-\n",
      "tion point, and allows to achieve very good per-\n",
      "formance both for en-de and en-ru. Thus, such\n",
      "pretraining can be an interesting technique in low-\n",
      "resource scenarios.\n",
      "\n",
      "BERT pretraining has been very successful in\n",
      "NLP. With respect to MT, it was shown to pro-\n",
      "vide better performance in Lample and Conneau\n",
      "(2019); Edunov et al. (2019) and allows to in-\n",
      "tegrate large source monolingual data in NMT\n",
      "model as opposed to target monolingual data usu-\n",
      "ally used for backtranslation.\n",
      "\n",
      "In this experimental study, we have shown that:\n",
      "\n",
      "• The next sentence prediction task in BERT\n",
      "is not necessary to improve performance - a\n",
      "masked LM task already is beneﬁcial.\n",
      "\n",
      "• It is beneﬁcial to train BERT on the source\n",
      "\n",
      "\fcorpora, therefore supporting the claim that\n",
      "pretraining the encoder provide a better in-\n",
      "tialization for NMT encoders.\n",
      "\n",
      "• Similar to Edunov et al. (2019), we observe\n",
      "that the impact of BERT pretraining is more\n",
      "important as the size of the training data de-\n",
      "creases (WMT vs IWSLT).\n",
      "\n",
      "• Information encoded by BERT is not sufﬁ-\n",
      "cient to perform the translation: NMT en-\n",
      "coder encodes both information speciﬁc to\n",
      "source sentence, and to the target sentence as\n",
      "well (cf the low performance of BERT frozen\n",
      "encoder).\n",
      "\n",
      "• Pretraining the encoder enables us to train\n",
      "bigger models.\n",
      "In IWSLT, the transformer\n",
      "big models were diverging, but when the en-\n",
      "coder is initialized with pretrained BERT the\n",
      "training became possible. For WMT14, train-\n",
      "ing a 12 layer encoder from scratch was prob-\n",
      "lematic, but News.Emb model (which con-\n",
      "tains 12 encoder layers) was trained and gave\n",
      "one of the best performances on WMT14.\n",
      "\n",
      "• Finetuning BERT pretrained encoder is more\n",
      "convenient : it leads to similar performance\n",
      "compared to reusing BERT as embedding\n",
      "layers, with faster decoding speed.\n",
      "\n",
      "• BERT pretrained models seem to be gener-\n",
      "ally better on different noise and domain test\n",
      "sets. However, we didn’t manage to obtain\n",
      "clear evidence that these models are more ro-\n",
      "bust.\n",
      "\n",
      "This experimental study was limited to a par-\n",
      "ticular dataset, language pair and model architec-\n",
      "ture. However, many other combinations are pos-\n",
      "sible. First, similar type of study needs to be per-\n",
      "formed with BERT pretrained model for NMT de-\n",
      "coder. Also, the model can be extended to other\n",
      "scenarios with BERT models such as Baevski et al.\n",
      "(2019). In addition, the comparison with ELMO\n",
      "embeddings is also interesting as in Edunov et al.\n",
      "(2019). Using embedding mostly inﬂuenced by\n",
      "neighboring words seems to echo the recent re-\n",
      "sults of convolutional self attention network (Yang\n",
      "et al., 2019). Using convolutional self attention\n",
      "network in BERT could bring additional beneﬁt\n",
      "for the pretrained representations. Another direc-\n",
      "tion could look at the impact of the number of lay-\n",
      "ers in BERT for NMT.\n",
      "\n",
      "Besides, one key question in this study was\n",
      "about the role of encoder in NMT as the roles of\n",
      "encoders and decoders are not clearly understood\n",
      "in current neural architectures. In the transformer\n",
      "architecture, the encoder probably computes some\n",
      "interlingual representations. In fact, nothing con-\n",
      "straints the model in reconstructing or predicting\n",
      "anything about the source sentences. If that is the\n",
      "case, why would a monolingual encoder help for\n",
      "the NMT task?\n",
      "\n",
      "One hypothesis is that encoders have a role of\n",
      "self encoding the sentences but also a translation\n",
      "effect by producing interlingual representations.\n",
      "In this case, a monolingual encoder could be a bet-\n",
      "ter starting point and could be seen as a regular-\n",
      "izer of the whole encoders. Another hypothesis is\n",
      "that the regularization of transformers models is\n",
      "not really effective and simply using BERT mod-\n",
      "els achieve this effect.\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this paper, we have compared different ways to\n",
      "use BERT language models for machine transla-\n",
      "tion. In particular, we have argued that the ben-\n",
      "eﬁt of using pretrained representations should not\n",
      "only be assessed in terms of BLEU score for the\n",
      "in-domain data but also in terms of generalization\n",
      "to new domains and in terms of robustness.\n",
      "\n",
      "Our experiments show that ﬁne-tuning the en-\n",
      "coder leads to comparable results as reusing the\n",
      "encoder as an additional embedding layers. How-\n",
      "ever,\n",
      "the former has an advantage of keeping\n",
      "the same model size as in standard NMT set-\n",
      "tings, while the latter adds additional parameters\n",
      "to the NMT model which increases signiﬁcantly\n",
      "the model size and might be critical in certain sce-\n",
      "narios.\n",
      "\n",
      "For MT practioners, using BERT has also sev-\n",
      "eral practical advantages beyond BLEU score.\n",
      "BERT can be trained for one source language and\n",
      "further reused for several translation pairs, thus\n",
      "providing a better initialization point for the mod-\n",
      "els and allowing for better performance.\n",
      "\n",
      "With respect to robustness tests, the conclusion\n",
      "are less clear. Even if pretrained BERT models\n",
      "obtained better performance on noisy test sets, it\n",
      "seems that they are not more robust than expected\n",
      "and that the potential of masked LM tasks is not\n",
      "fully exploited for machine translation. An inter-\n",
      "esting future work will be to assess the robustness\n",
      "of models from Song et al. (2019).\n",
      "\n",
      "\fReferences\n",
      "\n",
      "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke\n",
      "Zettlemoyer, and Michael Auli. 2019.\n",
      "Cloze-\n",
      "driven pretraining of self-attention networks. CoRR,\n",
      "abs/1903.07785.\n",
      "\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\n",
      "gio. 2014. Neural machine translation by jointly\n",
      "arXiv e-prints,\n",
      "learning to align and translate.\n",
      "abs/1409.0473.\n",
      "\n",
      "Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\n",
      "Christian Janvin. 2003. A neural probabilistic lan-\n",
      "guage model. J. Mach. Learn. Res., 3:1137–1155.\n",
      "\n",
      "Ondˇrej Bojar, Yvette Graham, and Amir Kamran.\n",
      "2017. Results of the wmt17 metrics shared task.\n",
      "In Proceedings of the Second Conference on Ma-\n",
      "chine Translation, Volume 2: Shared Task Papers,\n",
      "pages 489–513, Copenhagen, Denmark. Association\n",
      "for Computational Linguistics.\n",
      "\n",
      "Ronan Collobert and Jason Weston. 2008. A uniﬁed\n",
      "architecture for natural language processing: Deep\n",
      "In Pro-\n",
      "neural networks with multitask learning.\n",
      "ceedings of the 25th International Conference on\n",
      "Machine Learning, ICML 08, pages 160–167, New\n",
      "York, NY, USA. ACM.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2018. BERT: pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. CoRR, abs/1810.04805.\n",
      "\n",
      "Shuoyang Ding, Adithya Renduchintala, and Kevin\n",
      "Duh. 2019. A call for prudent choice of subword\n",
      "merge operations. CoRR, abs/1905.10453.\n",
      "\n",
      "Sergey Edunov, Alexei Baevski, and Michael Auli.\n",
      "2019. Pre-trained language model representations\n",
      "for language generation. CoRR, abs/1903.09722.\n",
      "\n",
      "Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, and\n",
      "Marjan Ghazvininejad. 2019. Training on synthetic\n",
      "noise improves robustness to natural noise in ma-\n",
      "chine translation. CoRR, abs/1902.01509.\n",
      "\n",
      "Guillaume Lample and Alexis Conneau. 2019. Cross-\n",
      "CoRR,\n",
      "language model pretraining.\n",
      "\n",
      "lingual\n",
      "abs/1901.07291.\n",
      "\n",
      "Jinhyuk Lee, Wonjin Yoon,\n",
      "\n",
      "Sungdong Kim,\n",
      "Donghyeon Kim, Sunkyu Kim, Chan Ho So,\n",
      "and Jaewoo Kang. 2019. Biobert: a pre-trained\n",
      "biomedical\n",
      "for\n",
      "biomedical text mining. CoRR, abs/1901.08746.\n",
      "\n",
      "language representation model\n",
      "\n",
      "Omer Levy and Yoav Goldberg. 2014. Neural word\n",
      "embedding as implicit matrix factorization. In Pro-\n",
      "ceedings of the 27th International Conference on\n",
      "Neural Information Processing Systems - Volume 2,\n",
      "NIPS’14, pages 2177–2185, Cambridge, MA, USA.\n",
      "MIT Press.\n",
      "\n",
      "Qingsong Ma, Ondej Bojar, and Yvette Graham. 2018.\n",
      "Results of the wmt18 metrics shared task: Both char-\n",
      "acters and embeddings achieve good performance.\n",
      "In Proceedings of the Third Conference on Machine\n",
      "Translation, Volume 2: Shared Task Papers, pages\n",
      "682–701, Belgium, Brussels. Association for Com-\n",
      "putational Linguistics.\n",
      "\n",
      "Paul Michel, Xian Li, Graham Neubig,\n",
      "\n",
      "and\n",
      "Juan Miguel Pino. 2019. On evaluation of ad-\n",
      "versarial perturbations for sequence-to-sequence\n",
      "models. CoRR, abs/1903.06620.\n",
      "\n",
      "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\n",
      "rado, and Jeffrey Dean. 2013. Distributed represen-\n",
      "tations of words and phrases and their composition-\n",
      "ality. In Proceedings of the 26th International Con-\n",
      "ference on Neural Information Processing Systems -\n",
      "Volume 2, NIPS’13, pages 3111–3119, USA. Curran\n",
      "Associates Inc.\n",
      "\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "Manning. 2014. Glove: Global vectors for word\n",
      "In Proceedings of the 2014 Con-\n",
      "representation.\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP), pages 1532–1543, Doha,\n",
      "Qatar. Association for Computational Linguistics.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "Zettlemoyer. 2018. Deep contextualized word rep-\n",
      "resentations. CoRR, abs/1802.05365.\n",
      "\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016a. Improving neural machine translation mod-\n",
      "In Proceedings of the\n",
      "els with monolingual data.\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers), pages\n",
      "86–96, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016b. Neural machine translation of rare words\n",
      "with subword units. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers), pages 1715–\n",
      "1725, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n",
      "Yan Liu. 2019. Mass: Masked sequence to sequence\n",
      "pre-training for language generation.\n",
      "\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.\n",
      "Sequence to sequence learning with neural net-\n",
      "In Proceedings of the 27th International\n",
      "works.\n",
      "Conference on Neural Information Processing Sys-\n",
      "tems - Volume 2, NIPS’14, pages 3104–3112, Cam-\n",
      "bridge, MA, USA. MIT Press.\n",
      "\n",
      "Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\n",
      "\n",
      "Bert rediscovers the classical nlp pipeline.\n",
      "\n",
      "Vaibhav, Sumeet Singh, Craig Stewart, and Gra-\n",
      "Improving robustness of ma-\n",
      "CoRR,\n",
      "\n",
      "ham Neubig. 2019.\n",
      "chine translation with synthetic noise.\n",
      "abs/1902.09508.\n",
      "\n",
      "\fAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In I. Guyon, U. V. Luxburg, S. Bengio,\n",
      "H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\n",
      "nett, editors, Advances in Neural Information Pro-\n",
      "cessing Systems 30, pages 5998–6008. Curran As-\n",
      "sociates, Inc.\n",
      "\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Fe-\n",
      "lix Hill, Omer Levy, and Samuel Bowman. 2018.\n",
      "GLUE: A multi-task benchmark and analysis plat-\n",
      "In Pro-\n",
      "form for natural language understanding.\n",
      "ceedings of\n",
      "the 2018 EMNLP Workshop Black-\n",
      "boxNLP: Analyzing and Interpreting Neural Net-\n",
      "works for NLP, pages 353–355, Brussels, Belgium.\n",
      "Association for Computational Linguistics.\n",
      "\n",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.\n",
      "Le, Mohammad Norouzi, Wolfgang Macherey,\n",
      "Maxim Krikun, Yuan Cao, Qin Gao, Klaus\n",
      "Macherey, Jeff Klingner, Apurva Shah, Melvin\n",
      "Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan\n",
      "Gouws, Yoshikiyo Kato, Taku Kudo, Hideto\n",
      "Kazawa, Keith Stevens, George Kurian, Nishant\n",
      "Patil, Wei Wang, Cliff Young, Jason Smith, Jason\n",
      "Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\n",
      "Macduff Hughes, and Jeffrey Dean. 2016. Google’s\n",
      "neural machine translation system: Bridging the gap\n",
      "between human and machine translation. CoRR,\n",
      "abs/1609.08144.\n",
      "\n",
      "Baosong Yang, Longyue Wang, Derek F. Wong,\n",
      "Lidia S. Chao, and Zhaopeng Tu. 2019. Con-\n",
      "CoRR,\n",
      "volutional\n",
      "abs/1904.03107.\n",
      "\n",
      "self-attention\n",
      "\n",
      "networks.\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "output_string = StringIO()\n",
    "with open(fname, 'rb') as in_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "print(output_string.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d34be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c63ac081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "fname = '1909.12744.pdf'\n",
    "fp = open(fname, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f7b5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obj(lt_objs, height):\n",
    "    #     print(lt_objs)\n",
    "    Lic = list()\n",
    "#     height = obj.bbox[3]\n",
    "    for obj in lt_objs:\n",
    "        if isinstance(obj, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            \n",
    "            x_0 = obj.bbox[0] \n",
    "            x_0,y_0,x_1,y_1= obj.bbox\n",
    "            box = (x_0, height-y_1, x_1, height-y_0)\n",
    "            box_integer = tuple([round(jj) for jj in obj.bbox])\n",
    "            txt = obj.get_text().replace('\\n', '')\n",
    "            x1,y1,x2,y2 = box_integer\n",
    "            Lic.append((x1,y1,x2,y2,txt))\n",
    "            \n",
    "    return Lic\n",
    "\n",
    "def issue_area_replace(page_info):\n",
    "    # 데이터 정렬 \n",
    "    sample = np.array(page_info).reshape(-1,5)\n",
    "    df = pd.DataFrame(sample,columns=['x1','y1','x2','y2','text'])\n",
    "    df = df.astype({'x1':'int','y1':'int','x2':'int','y2':'int','text':'str'})\n",
    "    df = df.sort_values(by=['y1','y2','x1'],ascending=(False,False,True))\n",
    "    \n",
    "    # text replace \n",
    "    stmp = 0\n",
    "    txt = list()\n",
    "    overlap_key = df['y1'].value_counts()[df['y1'].value_counts().values > 1].keys()\n",
    "\n",
    "    for idx, val in zip(df['text'].index, df['y1']):\n",
    "        if val in overlap_key:\n",
    "            if stmp != val:\n",
    "                stmp = val\n",
    "                txt_smb = df[df['y1'] ==val]['text'].tolist()\n",
    "                join_txt = \" \".join(txt_smb)\n",
    "                txt.append(join_txt)\n",
    "            else:pass\n",
    "        else:\n",
    "            txt.append(df['text'][idx])\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9239b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Page: 2\n",
      "Page: 3\n",
      "Page: 4\n",
      "Page: 5\n",
      "Page: 6\n",
      "Page: 7\n",
      "Page: 8\n",
      "Page: 9\n",
      "Page: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fname = '1909.12744.pdf'\n",
    "fp = open(fname, 'rb')\n",
    "\n",
    "# 여기서 부터가 진짜\n",
    "parser = PDFParser(fp)\n",
    "document = PDFDocument(parser)\n",
    "rsrcmgr = PDFResourceManager()\n",
    "device = PDFDevice(rsrcmgr)\n",
    "laparams = LAParams()\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "p_box = list()\n",
    "num = 1\n",
    "for page in PDFPage.create_pages(document):\n",
    "    print('Page:', num)\n",
    "    interpreter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "    height = layout.bbox[3]\n",
    "    page_info = parse_obj(layout._objs, height)\n",
    "    text_list = issue_area_replace(page_info)\n",
    "    replace_text = \"\\n\".join(text_list)\n",
    "#     print(replace_text)\n",
    "    p_box.append((num,replace_text))\n",
    "    num += 1\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af2f62ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page 1\n",
      "\n",
      "On the use of BERT for Neural Machine Translation\n",
      "St´ephane ClinchantNAVER LABS Europe, Francestephane.clinchant@naverlabs.com Vassilina NikoulinaNAVER LABS Europe, Francevassilina.nikoulina@naverlabs.com\n",
      "Kweon Woo JungNAVER Corp.,South Koreakweonwoo.jung@navercorp.com\n",
      "Abstract\n",
      "mostly shown improvements in low-resource orunsupervised NMT settings, and has been littlestudied in standard supervised scenario with rea-sonable amount of data available.\n",
      "9102\n",
      "peS72\n",
      "]LC.sc[\n",
      "Exploiting large pretrained models for var-ious NMT tasks have gained a lot of vis-In this work we studyibility recently.how BERT pretrained models could be ex-ploited for supervised Neural Machine Trans-lation. We compare various ways to in-tegrate pretrained BERT model with NMTmodel and study the impact of the mono-lingual data used for BERT training on theﬁnal translation quality. We use WMT-14English-German, IWSLT15 English-Germanand IWSLT14 English-Russian datasets forthese experiments. In addition to standard tasktest set evaluation, we perform evaluation onout-of-domain test sets and noise injected testsets, in order to assess how BERT pretrainedrepresentations affect model robustness.\n",
      "1 Introduction\n",
      "Current state of the art NMT models rely onthe Transformer model (Vaswani et al., 2017), afeed-forward network relying on attention mech-anism, which has surpassed prior state of the artarchitecture based on recurrent neural nets (Bah-danau et al., 2014; Sutskever et al., 2014). Beyondmachine translation, the transformer models havebeen reused to learn bi-directional language mod-els on large text corpora. The BERT model (De-vlin et al., 2018) consists in a transformer modelaiming at solving a masked language modellingtask, namely correctly predicting a masked wordfrom its context, and a next sentence predictiontask to decide whether two sentences are consecu-tive or not. In this work, we study how pretrainedBERT models can be exploited for transformer-based NMT, thus exploiting the fact that they relyon the same architecture.\n",
      "1v44721.9091:viXra\n",
      "The objective of this work is twofold. On onehand, we wish to perform systematic comparisonsof different BERT+NMT architectures for stan-dard supervised NMT. In addition, we argue thatthe beneﬁts of using pretrained representations hasbeen overlooked in previous studies and shouldbe assessed beyond BLEU scores on in-domainIn fact, LMs trained on huge datasetsdatasets.have the potentials of being more robust in generaland improve the performance for domain adapta-tion in MT.\n",
      "Pretrained Language Models (LM) such as ELMOand BERT (Peters et al., 2018; Devlin et al.,2018) have turned out to signiﬁcantly improvethe quality of several Natural Language Process-ing (NLP) tasks by transferring the prior knowl-edge learned from data-rich monolingual corporato data-poor NLP tasks such as question answer-ing, bio-medical information extraction and stan-dard benchmarks (Wang et al., 2018; Lee et al.,2019). In addition, it was shown that these rep-resentations contain syntactic and semantic infor-mation in different layers of the network (Tenneyet al., 2019). Therefore, using such pretrainedLMs for Neural Machine Translation (NMT) isappealing, and has been recently tried by severalpeople (Lample and Conneau, 2019; Edunov et al.,2019; Song et al., 2019).\n",
      "Unfortunately, the results of the\n",
      "In this study, we compare different ways to trainand reuse BERT for NMT. For instance, we showthat BERT can be trained only with a masked LMtask on the NMT source corpora and yield signif-icant improvement over the baseline. In addition,the models robustness is analyzed thanks to syn-thetic noise. above-mentioned works are not directly comparable toeach other as they used different methods, datasetsFurthermore, pretrained LMs haveand tasks.\n",
      "\n",
      "page 2\n",
      "\n",
      "The paper is organized as follows.\n",
      "In section2, we review relevant state of the art. Section 3enumerates different models we experiment with.Finally section 4 and 5 present our results beforediscussing the main contributions of this work insection 6.\n",
      "2 Related Works\n",
      "ELMO embedding are compared with the cloze-style BERT (Baevski et al., 2019) ones. The em-bedding network parameters are then either ﬁxed,or ﬁne-tuned. This work shows improvements onEnglish-German and English-Turkish translationtasks when using pretrained language model forsource word embedding initialization. However,the results are less clear when reusing embeddingon the target language side.\n",
      "Futhermore, Song et al.\n",
      "(2019) goes onestep further and proposes Masked Sequence-to-Sequence pretraining method. Rather than mask-ing a single token, it masks a sequence of tokenin the encoder and recovers them in the decoder.This model has shown new state of the art for un-supervised machine translation.\n",
      "The seminal work of (Bengio et al., 2003; Col-lobert and Weston, 2008) were one of the ﬁrst toshow that neural nets could learn word represen-tations useful in a variety of NLP tasks, pavingthe way for the word embedding era thanks toword2vec (Mikolov et al., 2013) and its vari-ants (Pennington et al., 2014; Levy and Goldberg,2014).\n",
      "Our work is an attempt to perform systematiccomparison on some of the aforementioned archi-tectures that incorporate pretrained LM in NMTmodel, concentrating on BERT pretrained LM rep-resentations applied on supervised machine trans-lation. However, we restrict ourselves to encoderpart only, and leave the decoder initialization forfuture work.\n",
      "With the recent advances and boost in perfor-mance of neural nets, ELMO (Peters et al., 2018)employed a Bi-LSTM network for language mod-elling and proposed to combine the different net-work layers to obtain effective word representa-tions. Shortly after the publication of ELMO,the BERT model (Devlin et al., 2018) was shownto have outstanding performance in various NLPtasks. Furthermore, the BERT model was reﬁnedin (Baevski et al., 2019) where the transformerself-attention mechanism is replaced by two di-rectional self-attention blocks: a left-to-right andright-to-left blocks are combined to predict themasked tokens.\n",
      "Regarding robustness, several recent studies(Karpukhin et al., 2019; Vaibhav et al., 2019) havetackled robustness issues with data augmentation.In this work, we study whether the robustnessproblem can be addressed at the model level ratherthan at data level. Michel et al. (2019) address ro-bustness problem with generative adversarial net-works. This method, as well as data augmentationmethods are complementary to our work and webelieve that they address different issues of robust-ness.\n",
      "3 Methods\n",
      "With respect to NMT, backtranslation (Sennrichet al., 2016a) is up to now one of the most effectiveways to exploit large monolingual data. However,backtranslation has the drawback of being onlyapplicable for target language data augmentation,while pretrained LMs can be used both for sourceand target language (independently (Edunov et al.,2019) or jointly (Lample and Conneau, 2019;Song et al., 2019)).\n",
      "Typical NMT model adopts the encoder-decoderarchitecture where the encoder forms contextual-ized word embedding from a source sentence andthe decoder generates a target translation from leftto right.\n",
      "Lample and Conneau (2019) initializes theentire encoder and decoder with a pretrainedMaskLM or Crosslingual MaskLM language mod-els trained on multilingual corpora. Such ini-tialization proved to be beneﬁcial for unsuper-vised machine translation, but also for English-Romanian supervised MT, bringing additional im-provements over standard backtranslation withMLM initialization.\n",
      "Pretrained LM, namely BERT, can inject priorknowledge on the encoder part of NMT, providingrich contextualized word embedding learned fromlarge monolingual corpus. Moreover, pretrainedLMs can be trained once, and reused for differentlanguage pairs1.\n",
      "1As opposed to backtranslation techniques which requires\n",
      "Edunov et al. (2019) uses ELMO (Peters et al.,2018) language model to set the word embed-thedings layer in NMT model. In addition, full NMT model retraining\n",
      "\n",
      "page 3\n",
      "\n",
      "In this study, we focus on reusing BERT mod-els for the NMT encoder2. We will compare thefollowing models:\n",
      "Moreover, since the BERT models are trainedto predict missing tokens from their context, theirrepresentations may also be more robust to miss-ing tokens or noisy inputs. We perform extensiverobustness study at section 4 verifying this hypoth-esis.\n",
      "• Baseline:. A transformer-big model withshared decoder input-output embedding pa-rameters.\n",
      "Finally,\n",
      "• Embedding (Emb):\n",
      "The baseline modelwhere the embedding layer is replaced by theBERT parameters (thus having 6 + 6 encoderlayers). The model is then ﬁne tuned simi-lar to the ELMO setting from (Edunov et al.,2019)\n",
      "language models trained on hugedatasets have the potentials of being more robustin general and improve the performance for do-main adaptation in MT. We therefore compareBERT models trained on different datasets, andperform evaluation on related test sets in order toassess the capacity of pretrained LMs on domainadaptation.\n",
      "4 WMT experiments\n",
      "• Fine-Tuning (FT): The baseline model withthe encoder initialized by the BERT parame-ters as in Lample and Conneau (2019)\n",
      "4.1 Preprocessing\n",
      "• Freeze: The baseline model with the en-coder initialized by the BERT parameters andfrozen. This means that the whole encoderhas been trained in purely monolingual set-tings, and only parameters responsible for thetranslation belong to the attention and de-coder models.\n",
      "We exploit the fact that BERT uses the samearchitecture as NMT encoder which allows us toinitialize NMT encoder with BERT pretrained pa-rameters. BERT pretraining has two advantagesover NMT training:\n",
      "We learn BPE (Sennrich et al., 2016b) model with32K split operations on the concatenation of Wikiand News corpus. This model is used both for Pre-trained LM subwords splitting and NMT source(English) side subwords splitting. German side ofNMT has been processed with 32K BPE modellearnt on target part of parallel corpus only. Pleasenote, that this is different from standard settingsfor WMT En-De experiments, which usually usesjoint BPE learning and shared source-target em-beddings. We do not adopt standard settings sinceit contradicts our original motivation for using pre-trained LM: English LM is learnt once and reusedfor different language pairs.\n",
      "4.2 Training\n",
      "• it solves a simpler (monolingual) task of‘source sentence encoding’, compared toNMT (bilingual task) which has to ‘encodesource sentence information’, and ‘translateinto a target language’.\n",
      "BERT For pretraining BERT models, we usethree different monolingual corpora of differentsizes and different domains. Table 1 summarizesthe statistics of these three monolingual corpora.\n",
      "• it has a possibility to exploit much largerdata, while NMT encoder is limited to sourceside of parallel corpus only.\n",
      "• NMT-src: source part of our parallel corpus\n",
      "that is used for NMT model training.\n",
      "• Wiki: English wikipedia dump\n",
      "• News:\n",
      "concatenation of 70M samplesfrom ”News Discussion”, ”News Crawl”and ”Common Crawl” English monolingualdatasets distributed by WMT-2019 sharedtask3. This resulted in total 210M samples.\n",
      "Even though the role of NMT encoder may gobeyond source sentence encoding (nothing pre-vents the model from encoding ‘translation re-lated’ information at the encoder level), better ini-tialization of encoder with BERT pretrained LMallows for faster NMT learning. Comparing set-tings where we freeze BERT parameters againstﬁne-tuning BERT allows to shed some light onthe capacity of the encoder/decoder model to learn‘translation-related’ information.\n",
      "The motivation of using NMT-src is to testwhether the resulting NMT model is more robust\n",
      "2Similar approach can be applied on the target language\n",
      "3http://www.statmt.org/wmt19/translation-task.html but we leave it for future work.\n",
      "\n",
      "page 4\n",
      "\n",
      "Lines Tokens4.5M 104M72M 2086M210M 3657M NMT-srcWikiNews\n",
      "Table 1: Monolingual (English) training data\n",
      "Lines Tok/line (en/de)300329972385500050005000 news14news18iwslt15OpenSubKDEwiki 19.7/18.319.5/18.316.4/15.46.3/5.58/7.717.7/15.5\n",
      "In/Out of Domain test sets.\n",
      "Table 2:news14 andnews18 are test sets from WMT-14 and WMT-18 newstranslation shared task.test set from IWSLT-iwslt:15 MT Track4. Wiki is randomly 5K sampled fromparallel Wikipedia distributed by OPUS5, OpenSub,KDE and Wiki are randomly 5K sampled from paral-lel Wikipedia, Open Subtitles and KDE corpora dis-tributed by OPUS6\n",
      "4.3 Evaluation\n",
      "We believe that the impact of pretrained LM inNMT model can not be measured by BLEU per-formance on in-domain test set only. Thereforewe introduce additional evaluation that allows tomeasure the impact of LM pretraining on differentout-of-domain tests. We also propose an evalua-tion procedure to evaluate the robustness to vari-ous types of noise for our models.\n",
      "Domain Besides standard WMT-14 news testset, models are evaluated on additional test setsgiven by Table 2. We include two in-domain(news) test sets, as well as additional out-of-domain test sets described in Table 2.\n",
      "Noise robustness. For robustness evaluation, weintroduce different type of noise to the standardnews14 test set:\n",
      "after having being trained on the source corpora.The Wiki corpora is bigger than the NMT-src butcould be classiﬁed as out-of-domain compared tonews dataset. Finally, the news dataset is thebiggest one and consists mostly of in-domain data.In all of our experiments, we only consider us-ing the masked LM task for BERT as the next sen-tence prediction tasks put restrictions on possibledata to use. We closely follow the masked LMtask described in (Devlin et al., 2018) with few ad-justments optimized for downstream NMT train-ing. We use frequency based sampling (Lampleand Conneau, 2019) in choosing 15% of tokens tomask, instead of uniformly sampling. Instead ofMASK token we used UNK token hoping that thustrained model will learn certain representation forunknowns that could be exploited by NMT model.Warm-up learning scheme described in (Vaswaniet al., 2017) results in faster convergence thanlinear decaying learning rate. The batch size of64000 tokens per batch is used, with maximum to-ken length of 250, half the original value, as we in-put single sentence only. We do not use [CLS] to-ken in the encoder side, as attention mechanism inNMT task can extract necessary information fromtoken-level representations. The BERT model isequivalent to the encoder side of Transformer Bigmodel. We train BERT model up to 200k iterationsuntil the accuracy for masked LM on developmentsaturates.\n",
      "NMT For NMT system training, we use WMT-14 English-German dataset.\n",
      "Typos: Similar to Karpukhin et al. (2019), weadd synthetic noise to the test set by randomly (1)swapping characters (chswap), (2) randomly in-serting or deleting characters (chrand), (3) upper-casing words (up). These test sets translations areevaluated against the golden news14 reference.\n",
      "Unk: An unknown character is introduced at thebeginning (noted UNK.S) or at the end of the sen-tence (noted UNK.E) before a punctuation symbolif any (this unknown character could be thoughtas as an unknown emoji, a character in differentscript, a rare unicode character). This token is in-troduced both for source and target sentence, andthe evaluation is performed with the augmented-reference.\n",
      "We use Transformer-Big as our baseline model.We share input embedding and output embed-ding parameters just before softmax on the de-coder side. Warm up learning scheme is usedwith warm-up steps of 4000. We use batch sizeof 32000 tokens per batch. Dropout of 0.3 is ap-plied to residual connections, and no dropout isapplied in attention layers. We decode with beamsize 4 with length penalty described in Wu et al.(2016). We conduct model selection with perplex-ity on development set. We average 5 checkpointsaround lowest perplexity. Intuitively, we expect the model to simply copyUNK token and proceed to the remaining tokens.\n",
      "\n",
      "page 5\n",
      "\n",
      "Interestingly, this simple test seems to producepoor translations, therefore puzzling the attentionand decoding process a lot. Table 3 gives an ex-ample of such translations for baseline model7.\n",
      "task. We believe, that the role of the NMT encoderis to encode both information speciﬁc to sourcesentence, but also information speciﬁc to the tar-get sentence (which is missing in BERT training).Next, we observe that even NMTSrc.FT (NMTencoder is initialized with BERT trained on sourcepart of parallel corpus) improves over the baseline.Note that this model uses the same amount of dataas the baseline. BERT task is simpler compared tothe task of the NMT encoder, but it is still related,thus BERT pretraining allows for a better initial-ization point for NMT model.\n",
      "Since the tasks are correlated, a better modelmight be better on noisy test sets as it behaves bet-ter in general. If we want to test that some modelsare indeed better, we need to disentangle this ef-fect and show that the gain in performance is notjust a random effect. A proper way would be tocompute the BLEU correlation between the orig-inal test set and the noisy versions but it wouldrequire a larger set of models for an accurate cor-relation estimation.\n",
      "When using more data for BERT training(Wiki.FT and News.FT), we gain even more im-provements over the baseline.\n",
      "∆(chrF) : We propose to look at the distribu-tion of the difference of sentence charf betweenthe noisy test set and the original test set. Indeed,looking at BLEU delta may not provide enough in-formation since it is corpus-level metric. Ideally,we would like to measure a number of sentencesor a margin for which we observe an ‘importantdecrease’ in translation quality. According to Maet al. (2018); Bojar et al. (2017), sentence levelchrF achieves good correlation with human judge-ments for En-De news translations.\n",
      "Finally, we observe comparable results forNews.Emb and News.FT (the difference in BLEUdoesn’t exceed 0.3 points, being higherforNews.FT on in-domain tests, and News.Emb forout-of-domain tests). Although News.FT conﬁg-uration keeps the size of the model same as stan-dard NMT system, News.Emb adds BERT param-eters to NMT parameters which doubles the sizeof NMT encoder. Additional encoder layers intro-duced in News.Emb does not add signiﬁcant value.\n",
      "More formally, let s be a sentence from the stan-dard news14 test set, n a noise operation, m atranslation model and r the reference sentence8:\n",
      "4.5 Robustness analysis\n",
      "∆(chrF)(m, n, s) = chrF(m(n(s)), r) −\n",
      "chrF(m(s), r) (1)\n",
      "In the analysis, we will report the distribution of∆(chrF) and its mean value as a summary. If amodel is good at dealing with noise, then the pro-duced sentence will be similar to the one producedby the noise-free input sentence. Therefore, the∆(chrF) will be closer to zero.\n",
      "4.4 Results\n",
      "Table 5 reports BLEU scores for the noisy test sets(described in section 4.3). As expected, we ob-serve an important drop in BLEU scores due tothe introduced noise. We observe that most pre-trained BERT models have better BLEU scorescompared to baseline for all type of noise (exceptNMTSrc.FT which suffers more from unknowntoken introduction in the end of the sentence com-pared to the Baseline). However, these results arenot enough to conclude, whether higher BLEUscores of BERT-augmented models are due to bet-ter robustness, or simply because these models areslightly better than the baseline in general.\n",
      "Table 4 presents the results of our experiments.As expected, freezing the encoder with BERT pa-rameters lead to a signiﬁcant decrease in transla-tion quality. However, other BERT+NMT archi-tectures mostly improve over the baseline both onin-domain and out-of-domain test sets. We con-clude, that the information encoded by BERT isuseful but not sufﬁcient to perform the translation\n",
      "7Output for (UNK.S+src) input is not an error, the model\n",
      "does produces an English sentence!\n",
      "8In the case of UNK transformation, the reference is\n",
      "This is why ﬁgure 1 reports the mean ∆(chrF)for several models. ∆(chrF) scores for UNKtests show that BERT models are not better thanexpected. However, for chswap, chrand, upper,the BERT models have a slightly lower ∆(chrF).Based on these results, we conclude that pretrain-ing the encoder with a masked LM task does notreally bring improvement in terms of robustnessto unknowns. It seems that BERT does yield im-provement for NMT as a better initialization for changed but we omit that to simplify the notation.\n",
      "\n",
      "page 6\n",
      "\n",
      "source sentence\n",
      "translation(src)\n",
      "translation(UNK.S + src)\n",
      "”In home cooking, there is much to be discovered - with a few minortweaks you can achieve good, if not sometimes better results,” said Proctor.”Beim Kochen zu Hause gibt es viel zu entdecken - mit ein paar kleinennderungen kann man gute, wenn nicht sogar manchmal bessereErgebnisse erzielen”, sagte Proktor.• ”In home cooking, there is much to be discovered - with a few minortweaks you can achieve good, if not sometimes better results”, sagte Proktor.\n",
      "Table 3: Example of a poor translation when adding unknown token to source sentences (translation done with abaseline transformer model)\n",
      "news1427.327.727.727.927.723.6 news1839.540.140.640.239.935.5 iwslt15 wiki17.618.318.418.818.915.0 kde OpenSub18.118.419.017.918.215.1 BaselineNMTsrc.FTWiki.FTNews.FTNews.EmbNews.Freeze 28.928.728.729.129.326.5 15.315.315.415.716.013.8\n",
      "Table 4: FT: initialize NMT encoder with BERT and ﬁnetune; Freeze: ﬁx NMT encoder parameters to BERTparameters; Emb: ﬁx encoder embeddding layer with BERT contextual word embeddings.\n",
      "German IWSLT 20159and English-RussianIWSLT 201410 MT track datasets. These arepretty small datasets (compared to previous exper-iments) which contain around 200K parallel sen-tences each.\n",
      "5.1 Experimental settings\n",
      "In these experiments we (1) reuse pretrainedBERT models from previous experiments or (2)train IWSLT BERT model. IWSLT BERT modelis trained on the concatenation of all the data avail-able at IWSLT 2014-2018 campaigns. After ﬁlter-ing out all the duplicates it contains around 780Ksentences and 13.8M tokens.\n",
      "Figure 1: Mean ∆(chrF) for several noisy test set andmodels. For the UNK test, the BERT models are sim-ilar or worst than the basline. For the chrand, chswap,upper, the BERT models are slightly better.\n",
      "NMT encoders but the full potential of maskedLM task is not fully exploited for NMT.\n",
      "5 IWSLT experiments\n",
      "We considered various settings for IWSLT base-line. First, for source side of the dataset, wetook 10K BPE merge operations, where BPEmodel was trained (1) either on the source side ofNMT data only, or (2) on all monolingual EnglishIWSLT data. Target side BPE uses 10K mergeoperations trained on the target side of the NMTdataset in all the IWSLST experiments. In our ﬁrstset of experiments, BPE model learnt on sourcedata only lead to similar translation performanceas BPE model learnt on all IWSLT English data.Therefore, in what follows we report results only\n",
      "9https://sites.google.com/site/iwsltevaluation2015/mt-\n",
      "track\n",
      "10https://sites.google.com/site/iwsltevaluation2014/mt-\n",
      "to explore the potential of maskedIn orderLM encoder pretraining for NMT in lower re-source settings, we train NMT models on English- track\n",
      "\n",
      "page 7\n",
      "\n",
      "ModelsBaselineNMTsrc.FTWiki.FTNews.FTNews.Emb news14 +UNK.S +UNK.E +chswap +chrand24.422.924.924.924.8 +up23.524.524.424.524.2 27.327.727.727.927.7 24.824.925.824.924.7 24.224.424.424.524.6 24.725.224.925.325.3\n",
      "Table 5: Robustness tests. BLEU scores for clean and ’noisiﬁed’ (with different noise type) news14 testset.\n",
      "en-de en-ru\n",
      "for the latter (referred as bpe10k).\n",
      "Baseline\n",
      "tbase.bpe10ktbase.dec3.bpe10k\n",
      "NMT model training on IWSLT datasets withTransformer Big architecture on IWSLT data hasdiverged both for en-de and en-ru dataset. There-fore we use Transformer Base (tbase) architectureas a baseline model for these experiments. IWSLTBERT model is also based on tbase architecturedescribed in Vaswani et al. (2017) and for the restfollows same training procedure as described inthe section 4.\n",
      "25.99.616.326.4BERT+NMT17.627.418.127.217.626.927.717.817.927.117.927.6 IWSLT.FT.tbase.bpe10kIWSLT.FT.tbase.dec3.bpe10kWiki.FT.tbig.bpe32kWiki.FT.tbig.dec3.bpe32kNews.FT.tbig.bpe32kNews.FT.tbig.dec3.bpe32k\n",
      "Table 6: IWSLT dataset results. IWSLT.FT: encoderis initialised with BERT model trained on IWSLTdata; tbase/tbig: transformer base/big architecture forNMT model; dec3: decoder layers reduced for 6 to3; bpe10k/bpe32k : amount of BPE merge operationsused for source language, learnt on the same dataset asBERT model (IWSLT or Wiki+News).\n",
      "In order to explore the potential of single pre-trained model for all language pairs/datasets wetry to reuse Wiki and News pretrained BERT mod-els from previous experiments for encoder initial-ization of NMT model. However, in the previousexperiments, our pretrained BERT models used32K BPE vocabulary and Transformer Big (tbig)architecture which means that we have to reuse thesame settings for the encoder trained on IWSLTdataset. It has been shown by Ding et al. (2019),these are not optimal settings for IWSLT train-ing because it leads to too many parameters forthe amount of data available. Therefore, in or-der to reduce the amount of the parameters of themodel, we also consider the case where we re-duce the amount of the decoder layers from 6 to3 (tbig.dec3).\n",
      "We do not observe big difference betweenIWSLT pretrained model and News/Wiki pre-trained model. We therefore may assume thatNews/Wiki BERT model can be considered as”general” English pretrained encoder, and be usedas a good starting point in any new model trans-lating from English (no matter target language ordomain).\n",
      "6 Discussion 5.2 Results\n",
      "BERT pretraining has been very successful inNLP. With respect to MT, it was shown to pro-vide better performance in Lample and Conneau(2019); Edunov et al. (2019) and allows to in-tegrate large source monolingual data in NMTmodel as opposed to target monolingual data usu-ally used for backtranslation.\n",
      "In this experimental study, we have shown that:\n",
      "• The next sentence prediction task in BERTis not necessary to improve performance - amasked LM task already is beneﬁcial.\n",
      "Table 6 reports the results of different sets of theexperiments on IWSLT data. First, we observethat BERT pretrained model improves over thebaseline, in any settings (BPE vocabulary, modelarchitecture, dataset used for pretraining).Inparticular, it is interesting to mention that with-out pretraining, both tbig.bpe32k and tbig.bpe10kmodels diverge when trained on IWSLT. How-ever, BERT pretraining gives a better initializa-tion point, and allows to achieve very good per-formance both for en-de and en-ru. Thus, suchpretraining can be an interesting technique in low-resource scenarios. • It is beneﬁcial to train BERT on the source\n",
      "\n",
      "page 8\n",
      "\n",
      "corpora, therefore supporting the claim thatpretraining the encoder provide a better in-tialization for NMT encoders.\n",
      "• Similar to Edunov et al. (2019), we observethat the impact of BERT pretraining is moreimportant as the size of the training data de-creases (WMT vs IWSLT).\n",
      "Besides, one key question in this study wasabout the role of encoder in NMT as the roles ofencoders and decoders are not clearly understoodin current neural architectures. In the transformerarchitecture, the encoder probably computes someinterlingual representations. In fact, nothing con-straints the model in reconstructing or predictinganything about the source sentences. If that is thecase, why would a monolingual encoder help forthe NMT task?\n",
      "• Information encoded by BERT is not sufﬁ-cient to perform the translation: NMT en-coder encodes both information speciﬁc tosource sentence, and to the target sentence aswell (cf the low performance of BERT frozenencoder).\n",
      "One hypothesis is that encoders have a role ofself encoding the sentences but also a translationeffect by producing interlingual representations.In this case, a monolingual encoder could be a bet-ter starting point and could be seen as a regular-izer of the whole encoders. Another hypothesis isthat the regularization of transformers models isnot really effective and simply using BERT mod-els achieve this effect.\n",
      "7 Conclusion\n",
      "• Pretraining the encoder enables us to trainbigger models.In IWSLT, the transformerbig models were diverging, but when the en-coder is initialized with pretrained BERT thetraining became possible. For WMT14, train-ing a 12 layer encoder from scratch was prob-lematic, but News.Emb model (which con-tains 12 encoder layers) was trained and gaveone of the best performances on WMT14.\n",
      "• Finetuning BERT pretrained encoder is moreconvenient : it leads to similar performancecompared to reusing BERT as embeddinglayers, with faster decoding speed.\n",
      "In this paper, we have compared different ways touse BERT language models for machine transla-tion. In particular, we have argued that the ben-eﬁt of using pretrained representations should notonly be assessed in terms of BLEU score for thein-domain data but also in terms of generalizationto new domains and in terms of robustness.\n",
      "• BERT pretrained models seem to be gener-ally better on different noise and domain testsets. However, we didn’t manage to obtainclear evidence that these models are more ro-bust.\n",
      "Our experiments show that ﬁne-tuning the en-coder leads to comparable results as reusing theencoder as an additional embedding layers. How-ever,the former has an advantage of keepingthe same model size as in standard NMT set-tings, while the latter adds additional parametersto the NMT model which increases signiﬁcantlythe model size and might be critical in certain sce-narios.\n",
      "For MT practioners, using BERT has also sev-eral practical advantages beyond BLEU score.BERT can be trained for one source language andfurther reused for several translation pairs, thusproviding a better initialization point for the mod-els and allowing for better performance.\n",
      "This experimental study was limited to a par-ticular dataset, language pair and model architec-ture. However, many other combinations are pos-sible. First, similar type of study needs to be per-formed with BERT pretrained model for NMT de-coder. Also, the model can be extended to otherscenarios with BERT models such as Baevski et al.(2019). In addition, the comparison with ELMOembeddings is also interesting as in Edunov et al.(2019). Using embedding mostly inﬂuenced byneighboring words seems to echo the recent re-sults of convolutional self attention network (Yanget al., 2019). Using convolutional self attentionnetwork in BERT could bring additional beneﬁtfor the pretrained representations. Another direc-tion could look at the impact of the number of lay-ers in BERT for NMT. With respect to robustness tests, the conclusionare less clear. Even if pretrained BERT modelsobtained better performance on noisy test sets, itseems that they are not more robust than expectedand that the potential of masked LM tasks is notfully exploited for machine translation. An inter-esting future work will be to assess the robustnessof models from Song et al. (2019).\n",
      "\n",
      "page 9\n",
      "\n",
      "References\n",
      "Alexei Baevski, Sergey Edunov, Yinhan Liu, LukeZettlemoyer, and Michael Auli. 2019.Cloze-driven pretraining of self-attention networks. CoRR,abs/1903.07785.\n",
      "Qingsong Ma, Ondej Bojar, and Yvette Graham. 2018.Results of the wmt18 metrics shared task: Both char-acters and embeddings achieve good performance.In Proceedings of the Third Conference on MachineTranslation, Volume 2: Shared Task Papers, pages682–701, Belgium, Brussels. Association for Com-putational Linguistics.\n",
      "Paul Michel, Xian Li, Graham Neubig,\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2014. Neural machine translation by jointlyarXiv e-prints,learning to align and translate.abs/1409.0473.\n",
      "andJuan Miguel Pino. 2019. On evaluation of ad-versarial perturbations for sequence-to-sequencemodels. CoRR, abs/1903.06620.\n",
      "Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, andChristian Janvin. 2003. A neural probabilistic lan-guage model. J. Mach. Learn. Res., 3:1137–1155.\n",
      "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean. 2013. Distributed represen-tations of words and phrases and their composition-ality. In Proceedings of the 26th International Con-ference on Neural Information Processing Systems -Volume 2, NIPS’13, pages 3111–3119, USA. CurranAssociates Inc.\n",
      "Ondˇrej Bojar, Yvette Graham, and Amir Kamran.2017. Results of the wmt17 metrics shared task.In Proceedings of the Second Conference on Ma-chine Translation, Volume 2: Shared Task Papers,pages 489–513, Copenhagen, Denmark. Associationfor Computational Linguistics.\n",
      "Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. Glove: Global vectors for wordIn Proceedings of the 2014 Con-representation.ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1532–1543, Doha,Qatar. Association for Computational Linguistics.\n",
      "Ronan Collobert and Jason Weston. 2008. A uniﬁedarchitecture for natural language processing: DeepIn Pro-neural networks with multitask learning.ceedings of the 25th International Conference onMachine Learning, ICML 08, pages 160–167, NewYork, NY, USA. ACM.\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, MattGardner, Christopher Clark, Kenton Lee, and LukeZettlemoyer. 2018. Deep contextualized word rep-resentations. CoRR, abs/1802.05365.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. BERT: pre-training ofdeep bidirectional transformers for language under-standing. CoRR, abs/1810.04805.\n",
      "Shuoyang Ding, Adithya Renduchintala, and KevinDuh. 2019. A call for prudent choice of subwordmerge operations. CoRR, abs/1905.10453.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.2016a. Improving neural machine translation mod-In Proceedings of theels with monolingual data.54th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages86–96, Berlin, Germany. Association for Computa-tional Linguistics.\n",
      "Sergey Edunov, Alexei Baevski, and Michael Auli.2019. Pre-trained language model representationsfor language generation. CoRR, abs/1903.09722.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.2016b. Neural machine translation of rare wordswith subword units. In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for Computa-tional Linguistics.\n",
      "Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, andMarjan Ghazvininejad. 2019. Training on syntheticnoise improves robustness to natural noise in ma-chine translation. CoRR, abs/1902.01509.\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to sequencepre-training for language generation. Guillaume Lample and Alexis Conneau. 2019. Cross-CoRR,language model pretraining.\n",
      "lingualabs/1901.07291.\n",
      "Jinhyuk Lee, Wonjin Yoon,\n",
      "language representation model\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.Sequence to sequence learning with neural net-In Proceedings of the 27th Internationalworks.Conference on Neural Information Processing Sys-tems - Volume 2, NIPS’14, pages 3104–3112, Cam-bridge, MA, USA. MIT Press.\n",
      "Sungdong Kim,Donghyeon Kim, Sunkyu Kim, Chan Ho So,and Jaewoo Kang. 2019. Biobert: a pre-trainedbiomedicalforbiomedical text mining. CoRR, abs/1901.08746.\n",
      "Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\n",
      "Bert rediscovers the classical nlp pipeline.\n",
      "Vaibhav, Sumeet Singh, Craig Stewart, and Gra-Improving robustness of ma-CoRR,\n",
      "Omer Levy and Yoav Goldberg. 2014. Neural wordembedding as implicit matrix factorization. In Pro-ceedings of the 27th International Conference onNeural Information Processing Systems - Volume 2,NIPS’14, pages 2177–2185, Cambridge, MA, USA.MIT Press. ham Neubig. 2019.chine translation with synthetic noise.abs/1902.09508.\n",
      "\n",
      "page 10\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, Ł ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In I. Guyon, U. V. Luxburg, S. Bengio,H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-nett, editors, Advances in Neural Information Pro-cessing Systems 30, pages 5998–6008. Curran As-sociates, Inc.\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Fe-lix Hill, Omer Levy, and Samuel Bowman. 2018.GLUE: A multi-task benchmark and analysis plat-In Pro-form for natural language understanding.ceedings ofthe 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Net-works for NLP, pages 353–355, Brussels, Belgium.Association for Computational Linguistics.\n",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.Le, Mohammad Norouzi, Wolfgang Macherey,Maxim Krikun, Yuan Cao, Qin Gao, KlausMacherey, Jeff Klingner, Apurva Shah, MelvinJohnson, Xiaobing Liu, Lukasz Kaiser, StephanGouws, Yoshikiyo Kato, Taku Kudo, HidetoKazawa, Keith Stevens, George Kurian, NishantPatil, Wei Wang, Cliff Young, Jason Smith, JasonRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,Macduff Hughes, and Jeffrey Dean. 2016. Google’sneural machine translation system: Bridging the gapbetween human and machine translation. CoRR,abs/1609.08144.\n",
      "self-attention networks.\n",
      "Baosong Yang, Longyue Wang, Derek F. Wong,Lidia S. Chao, and Zhaopeng Tu. 2019. Con-CoRR,volutionalabs/1904.03107.\n"
     ]
    }
   ],
   "source": [
    "for i,j in p_box:\n",
    "    print( )\n",
    "    print('page',i)\n",
    "    print( )\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좌우 워터 마크 제거\n",
    "def parse_obj(lt_objs, b_size):\n",
    "    Lic = list()\n",
    "    a, b, c, d = b_size\n",
    "    add_integer = tuple([round(jj) for jj in (a+50,b+50,c-50,d-50)])\n",
    "    \n",
    "    for obj in lt_objs:\n",
    "        if obj.bbox[0] >add_integer[0]  and obj.bbox[2] >add_integer[0] and obj.bbox[1] <add_integer[-1] and obj.bbox[3] <add_integer[-1]  : \n",
    "            if isinstance(obj, pdfminer.layout.LTTextBoxHorizontal):\n",
    "\n",
    "                x_0 = obj.bbox[0] \n",
    "                x_0,y_0,x_1,y_1= obj.bbox\n",
    "                box = (x_0, d-y_1, x_1, d-y_0)\n",
    "                box_integer = tuple([round(jj) for jj in obj.bbox])\n",
    "                txt = obj.get_text().replace('\\n', '')\n",
    "                x1,y1,x2,y2 = box_integer\n",
    "                Lic.append((x1,y1,x2,y2,txt))\n",
    "            \n",
    "    return Lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1485c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca4d7c233ae746d13439eeb512b95cbaaa636f290f4feea89042edff8b08a19a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('EPT_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
